{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 10\n",
      "Probabilities:\n",
      "  apple       : 0.361\n",
      "  banana      : 0.081\n",
      "  cherry      : 0.163\n",
      "  date        : 0.285\n",
      "  elderberry  : 0.110\n",
      "Mean Reward: 0.500\n",
      "KL Divergence: 0.001\n",
      "\n",
      "Iteration 20\n",
      "Probabilities:\n",
      "  apple       : 0.613\n",
      "  banana      : 0.056\n",
      "  cherry      : 0.094\n",
      "  date        : 0.131\n",
      "  elderberry  : 0.106\n",
      "Mean Reward: 0.650\n",
      "KL Divergence: 0.002\n",
      "\n",
      "Iteration 30\n",
      "Probabilities:\n",
      "  apple       : 0.775\n",
      "  banana      : 0.033\n",
      "  cherry      : 0.052\n",
      "  date        : 0.058\n",
      "  elderberry  : 0.082\n",
      "Mean Reward: 0.950\n",
      "KL Divergence: 0.000\n",
      "\n",
      "Iteration 40\n",
      "Probabilities:\n",
      "  apple       : 0.861\n",
      "  banana      : 0.019\n",
      "  cherry      : 0.032\n",
      "  date        : 0.030\n",
      "  elderberry  : 0.058\n",
      "Mean Reward: 0.850\n",
      "KL Divergence: 0.000\n",
      "\n",
      "Iteration 50\n",
      "Probabilities:\n",
      "  apple       : 0.918\n",
      "  banana      : 0.011\n",
      "  cherry      : 0.017\n",
      "  date        : 0.016\n",
      "  elderberry  : 0.038\n",
      "Mean Reward: 0.900\n",
      "KL Divergence: 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax, kl_div\n",
    "\n",
    "# 词汇表及其分组\n",
    "vocab = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\n",
    "is_vowel = lambda x: x[0].lower() in \"aeiou\"\n",
    "\n",
    "class GRPO:\n",
    "    def __init__(self, vocab, beta=0.1, epsilon=1e-8):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.beta = beta  # KL散度的权重系数\n",
    "        self.epsilon = epsilon  # 防止除零\n",
    "        \n",
    "        # 初始化策略参数(logits)\n",
    "        np.random.seed(42)\n",
    "        self.theta = np.random.randn(self.vocab_size)\n",
    "        \n",
    "    def get_policy_probs(self, logits):\n",
    "        \"\"\"获取策略概率分布\"\"\"\n",
    "        return softmax(logits)\n",
    "    \n",
    "    def sample_word(self, probs):\n",
    "        \"\"\"从概率分布中采样一个词\"\"\"\n",
    "        word_idx = np.random.choice(len(self.vocab), p=probs)\n",
    "        return word_idx\n",
    "    \n",
    "    def compute_reward(self, word_idx):\n",
    "        \"\"\"计算奖励（元音开头为1.0，否则为0.0）\"\"\"\n",
    "        word = self.vocab[word_idx]\n",
    "        return 1.0 if is_vowel(word) else 0.0\n",
    "    \n",
    "    def compute_kl_divergence(self, old_probs, new_probs):\n",
    "        \"\"\"计算KL散度\"\"\"\n",
    "        return np.sum(kl_div(old_probs, new_probs))\n",
    "    \n",
    "    def train(self, num_iterations=50, num_samples=20):\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "            # 1. 输入查询q（这里我们的查询是隐式的，就是选择单词的任务）\n",
    "            current_probs = self.get_policy_probs(self.theta)\n",
    "            \n",
    "            # 2. 收集样本和计算奖励\n",
    "            outputs = []  # 存储(word_idx, reward)\n",
    "            rewards = []\n",
    "            \n",
    "            for _ in range(num_samples):\n",
    "                word_idx = self.sample_word(current_probs)\n",
    "                reward = self.compute_reward(word_idx)\n",
    "                outputs.append((word_idx, reward))\n",
    "                rewards.append(reward)\n",
    "                \n",
    "            # 3. 组内统计计算\n",
    "            rewards = np.array(rewards)\n",
    "            mu_G = np.mean(rewards)  # 奖励均值\n",
    "            sigma_G = np.std(rewards) + self.epsilon  # 奖励标准差\n",
    "            \n",
    "            # 4. 计算每个样本的相对优势\n",
    "            advantages = [(r - mu_G) / (sigma_G) for _, r in outputs]\n",
    "            \n",
    "            # 5. 策略优化\n",
    "            grad = np.zeros_like(self.theta)\n",
    "            \n",
    "            for (word_idx, _), advantage in zip(outputs, advantages):\n",
    "                # 计算策略梯度\n",
    "                grad_sample = -current_probs.copy()\n",
    "                grad_sample[word_idx] += 1.0\n",
    "                grad += grad_sample * advantage\n",
    "            \n",
    "            # 更新策略参数\n",
    "            learning_rate = 0.01\n",
    "            self.theta += learning_rate * grad\n",
    "            \n",
    "            # 打印当前迭代的统计信息\n",
    "            new_probs = self.get_policy_probs(self.theta)\n",
    "            kl_div_value = self.compute_kl_divergence(current_probs, new_probs)\n",
    "            \n",
    "            if iteration % 10 == 0:\n",
    "                print(f\"\\nIteration {iteration}\")\n",
    "                print(\"Probabilities:\")\n",
    "                for word, prob in zip(vocab, new_probs):\n",
    "                    print(f\"  {word:12s}: {prob:.3f}\")\n",
    "                print(f\"Mean Reward: {mu_G:.3f}\")\n",
    "                print(f\"KL Divergence: {kl_div_value:.3f}\")\n",
    "\n",
    "# 运行训练\n",
    "grpo = GRPO(vocab)\n",
    "grpo.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
